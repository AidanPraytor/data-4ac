#!apt install libspatialindex-dev --quiet
get_ipython().getoutput("pip install rtree --quiet")
get_ipython().getoutput("pip install geopandas --quiet")
get_ipython().getoutput("pip install folium --quiet")


import warnings
warnings.filterwarnings('ignore')

from datascience import *
import pandas as pd
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
import folium

from sklearn.preprocessing import normalize

get_ipython().run_line_magic("matplotlib", " inline")
plt.style.use('fivethirtyeight')


from google.colab import drive
drive.mount('/content/drive')


alpr = Table().read_table("/content/drive/MyDrive/Colab Notebooks/ALPR/lprs-data.csv")
alpr


def getlatitude(s):
    before, after = s.split(',') # Break it into two parts
    latstring = before[1:] # Get rid of the annoying '('
    return float(latstring) # Convert the string to a number
def getlongitude(s):
    before, after = s.split(',') # Break it into two parts
    longstring = after[1:-1] # Get rid of the ' ' and the ')'
    return float(longstring) # Convert the string to a number


# Relabel columns 
alpr = alpr.relabeled("red_VRM", "Plate").relabeled("red_Timestamp", "Timestamp")

# Split Location to 'Latitude' and 'Longitude' columns
alpr = alpr.with_columns("Latitude", alpr.apply(getlatitude, "Location 1"), "Longitude", alpr.apply(getlongitude, "Location 1")).drop("Location 1")

# Sort the LPRS data by Timestamp in chronological order
alpr = alpr.sort("Timestamp", descending=False)


alpr


plate_counts = alpr.group("Plate").sort("count", descending=True).relabel("count", "times seen")


plate_counts


plate_counts.hist('times seen', bins=np.arange(0, 10, 1))


plate_counts_above_10 = plate_counts.where("times seen", are.above_or_equal_to(10)).num_rows


plate_counts_above_10


#Append time seen count to alpr object


type(alpr)


#this converts our datascience table into a pandas dataframe that we can then turn to a geopandas df
table_df = alpr.to_df()


# storing this dataframe in a csv file
#table_df.to_csv('/content/alpr.csv', index = None)


#this converts our pandas dataframe into a Geopandas one
alprgs = gpd.GeoDataFrame(
    table_df, geometry=gpd.points_from_xy(table_df.Longitude, table_df.Latitude, crs = 'EPSG:4326'))


#from folium.plugins import FastMarkerCluster
#Define coordinates of where we want to center our map
bay_coords = [37.871545, -122.260807]

#Create the map
my_map = folium.Map(
    location = bay_coords,
    tiles='Stamen Toner',
    zoom_start = 5)
    
points = folium.features.GeoJson(alprgs)
my_map.add_child(folium.plugins.FastMarkerCluster(alprgs[['Latitude', 'Longitude']].values.tolist()))

#Display the map
display(my_map)

#to save the map, perhaps for embedding in a website or presentation, you could use this code
#my_map.save("save_file.html")


#Find and delete lat long that == 0 for ALPR





indicators = Table().read_table("/content/drive/MyDrive/Colab Notebooks/ALPR/indicators.csv")
indicators


demographics = Table().read_table("/content/drive/MyDrive/Colab Notebooks/ALPR/demographics.csv")
demographics


oakland_indicators = indicators.where("Approximate Location", are.equal_to("Oakland"))


oakland = oakland_indicators.join("Census Tract", demographics)


oakland


oakland.hist("Total Population", bins=np.arange(0, 8000, 500))


oakland.hist("Traffic")


oakland.hist("Hispanic (%)", "White (%)", bins=np.arange(0, 100, 10))


tracts = gpd.read_file("/content/drive/MyDrive/Colab Notebooks/ALPR/Alameda_CTracts.shp")


#add leading 0 to tabular data column to
oakland = oakland["Census Tract"].str.zfill(11)


# Needs to get fixed... regardless, we don't expect students to understand this and we don't want them to. 
# The following are functions that add to the map functionality and expression.

def get_colors_from_column(tbl, col, include_outliers=False):
    """Assigns each row of the input table to a color based on the value of its percentage column."""
    vmin = min(tbl.column(col))
    vmax = max(tbl.column(col))

    if include_outliers:
        outlier_min_bound = vmin
        outlier_max_bound = vmax
    else:
        q1 = np.percentile(tbl.column(col), 25)
        q3 = np.percentile(tbl.column(col), 75)
        IQR = q3 - q1
        outlier_min_bound = max(vmin, q1 - 1.5 * IQR)
        outlier_max_bound = min(vmax, q3 + 1.5 * IQR)
        
    colorbar_scale = list(np.linspace(outlier_min_bound, outlier_max_bound, 10))
    scale_colors = ['#006100', '#3c8000', '#6ba100', '#a3c400', '#dfeb00', '#ffea00', '#ffbb00', '#ff9100', '#ff6200', '#ff2200']
    
    def assign_color(colors, cutoffs, datapoint):
        """Assigns a color to the input percent based on the data's distribution."""
        for i, cutoff in enumerate(cutoffs):
            if cutoff >= datapoint:
                return colors[i - 1] if i > 0 else colors[0]
        return colors[-1]
    
    colors = [""] * tbl.num_rows
    for i, datapoint in enumerate(tbl.column(col)): 
        colors[i] = assign_color(scale_colors, colorbar_scale, datapoint)
        
    return colors


# Needs to be fixed... ideally clicking on a dot should present 'text' as shown in tbl line.
# The idea is that the other features and information are present (e.g., population demographics, pop size).

def map_feature(feature):
    
    colors = get_colors_from_column(oakland, feature, include_outliers=True) # Get colors based on percentiles
    
    # Note: normalize data then scale to increase
    factor = np.linalg.norm(oakland.column(feature))
    areas = (oakland.column(feature) / factor) * 10000
    
    tbl = oakland.select('Latitude', 'Longitude').with_columns('type', 'text', 'colors', colors, 'areas', areas)
    
    return tbl


size_map(map_feature('Hispanic (%)'))


size_map(map_feature('Traffic'))









